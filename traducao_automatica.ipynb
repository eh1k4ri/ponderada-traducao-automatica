{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importações"
      ],
      "metadata": {
        "id": "isHtwJN6WjRh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.23.5 d2l==1.0.3 --quiet"
      ],
      "metadata": {
        "id": "sH6qAEFGVeMe"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from d2l import torch as d2l\n",
        "import torch\n",
        "from torch import nn"
      ],
      "metadata": {
        "id": "8xobdqSuWdV8"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download e Leitura dos dados"
      ],
      "metadata": {
        "id": "xn4VmMo3WuOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data_nmt():\n",
        "    \"\"\"Carrega o dataset Inglês-Francês do Projeto Tatoeba.\"\"\"\n",
        "    d2l.DATA_HUB['fra-eng'] = (\n",
        "        d2l.DATA_URL + 'fra-eng.zip',\n",
        "        '94646ad1522d915e7b0f9296181140edcf86a4f5')\n",
        "    data_dir = d2l.download_extract('fra-eng')\n",
        "    with open(os.path.join(data_dir, 'fra.txt'), 'r', encoding='utf-8') as f:\n",
        "        return f.read()"
      ],
      "metadata": {
        "id": "hIA-Q6iHW41h"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pré-processamento"
      ],
      "metadata": {
        "id": "-whIzK7LW6aU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_nmt(text):\n",
        "    def no_space(char, prev_char):\n",
        "        return char in set(',.!?') and prev_char != ' '\n",
        "\n",
        "    text = text.replace('\\u202f', ' ').replace('\\xa0', ' ').lower()\n",
        "    out = [' ' + char if i > 0 and no_space(char, text[i - 1]) else char\n",
        "           for i, char in enumerate(text)]\n",
        "    return ''.join(out)"
      ],
      "metadata": {
        "id": "cHa7ZrfDW9TC"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenização"
      ],
      "metadata": {
        "id": "EBF04xDeXC2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_nmt(text, num_examples=None):\n",
        "    source, target = [], []\n",
        "    for i, line in enumerate(text.split('\\n')):\n",
        "        if num_examples and i >= num_examples:\n",
        "            break\n",
        "        parts = line.split('\\t')\n",
        "        if len(parts) == 2:\n",
        "            source.append(parts[0].split(' '))\n",
        "            target.append(parts[1].split(' '))\n",
        "    return source, target"
      ],
      "metadata": {
        "id": "EwNnziEMXFxY"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Truncar ou preencher"
      ],
      "metadata": {
        "id": "oXJlHZ4OXTXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def truncate_pad(line, num_steps, padding_token):\n",
        "    if len(line) > num_steps:\n",
        "        return line[:num_steps]\n",
        "    return line + [padding_token] * (num_steps - len(line))"
      ],
      "metadata": {
        "id": "JC1X7JJRXWZG"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformar para array"
      ],
      "metadata": {
        "id": "qDlm41h_XaT7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_array_nmt(lines, vocab, num_steps):\n",
        "    lines = [vocab[l] for l in lines]\n",
        "    lines = [l + [vocab['<eos>']] for l in lines]\n",
        "    array = torch.tensor([truncate_pad(\n",
        "        l, num_steps, vocab['<pad>']) for l in lines])\n",
        "    valid_len = (array != vocab['<pad>']).type(torch.int32).sum(1)\n",
        "    return array, valid_len"
      ],
      "metadata": {
        "id": "wVPTU8N2Xdfy"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load do dataset"
      ],
      "metadata": {
        "id": "zaXueNN1Xq3f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data_nmt(batch_size, num_steps, num_examples=600):\n",
        "    text = preprocess_nmt(read_data_nmt())\n",
        "    source, target = tokenize_nmt(text, num_examples)\n",
        "    src_vocab = d2l.Vocab(source, min_freq=2,\n",
        "                          reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
        "    tgt_vocab = d2l.Vocab(target, min_freq=2,\n",
        "                          reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
        "    src_array, src_valid_len = build_array_nmt(source, src_vocab, num_steps)\n",
        "    tgt_array, tgt_valid_len = build_array_nmt(target, tgt_vocab, num_steps)\n",
        "    data_arrays = (src_array, src_valid_len, tgt_array, tgt_valid_len)\n",
        "    data_iter = d2l.load_array(data_arrays, batch_size)\n",
        "    return data_iter, src_vocab, tgt_vocab"
      ],
      "metadata": {
        "id": "KCIPGqJBXsb-"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Teste para o exercício 9.5.7 - 1"
      ],
      "metadata": {
        "id": "KvIoslNbXxzx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTJ1Heq4U-B6",
        "outputId": "43e008cb-a273-47d0-ebc3-7bb7fc622c08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100 exemplos → vocabulário origem: 40, destino: 39\n",
            "1000 exemplos → vocabulário origem: 266, destino: 321\n",
            "5000 exemplos → vocabulário origem: 875, destino: 1231\n"
          ]
        }
      ],
      "source": [
        "batch_size, num_steps = 32, 10\n",
        "device = d2l.try_gpu()\n",
        "train_iter, src_vocab, tgt_vocab = load_data_nmt(batch_size, num_steps)\n",
        "\n",
        "for n in [100, 1000, 5000]:\n",
        "    _, src_vocab_n, tgt_vocab_n = load_data_nmt(batch_size=2, num_steps=10, num_examples=n)\n",
        "    print(f\"{n} exemplos → vocabulário origem: {len(src_vocab_n)}, destino: {len(tgt_vocab_n)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelo Encoder-Decoder"
      ],
      "metadata": {
        "id": "ZX2Mkt1kYPiU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2SeqEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.rnn = nn.GRU(embed_size, num_hiddens, num_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, X):\n",
        "        embedded = self.embedding(X)\n",
        "        output, state = self.rnn(embedded)\n",
        "        return output, state\n",
        "\n",
        "class Seq2SeqDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.rnn = nn.GRU(embed_size, num_hiddens, num_layers, batch_first=True)\n",
        "        self.dense = nn.Linear(num_hiddens, vocab_size)\n",
        "\n",
        "    def forward(self, X, state):\n",
        "        embedded = self.embedding(X)\n",
        "        output, state = self.rnn(embedded, state)\n",
        "        return self.dense(output), state\n",
        "\n",
        "class EncoderDecoder(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, enc_X, dec_X):\n",
        "        enc_outputs, enc_state = self.encoder(enc_X)\n",
        "        dec_outputs, _ = self.decoder(dec_X, enc_state)\n",
        "        return dec_outputs"
      ],
      "metadata": {
        "id": "ufvzlLxEYUDf"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inicialização e Treino"
      ],
      "metadata": {
        "id": "Py_mw7fCYZau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size, num_hiddens, num_layers = 32, 32, 2\n",
        "encoder = Seq2SeqEncoder(len(src_vocab), embed_size, num_hiddens, num_layers)\n",
        "decoder = Seq2SeqDecoder(len(tgt_vocab), embed_size, num_hiddens, num_layers)\n",
        "model = EncoderDecoder(encoder, decoder).to(device)\n",
        "\n",
        "def init_weights(m):\n",
        "    if type(m) == nn.Linear or type(m) == nn.Embedding:\n",
        "        nn.init.xavier_uniform_(m.weight)\n",
        "model.apply(init_weights)\n",
        "\n",
        "loss = nn.CrossEntropyLoss(ignore_index=tgt_vocab['<pad>'])\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)"
      ],
      "metadata": {
        "id": "zEdmvMkEYX47"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loop de treinamento"
      ],
      "metadata": {
        "id": "6lppeuTmYhKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for X, X_valid_len, Y, Y_valid_len in train_iter:\n",
        "        X, Y = X.to(device), Y.to(device)\n",
        "        bos = torch.tensor([tgt_vocab['<bos>']] * Y.shape[0], device=device).reshape(-1, 1)\n",
        "        dec_input = torch.cat([bos, Y[:, :-1]], 1)\n",
        "        pred = model(X, dec_input)\n",
        "        l = loss(pred.reshape(-1, pred.shape[-1]), Y.reshape(-1))\n",
        "        optimizer.zero_grad()\n",
        "        l.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += l.item()\n",
        "    print(f'Epoch {epoch + 1}, Loss {total_loss:.3f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEP0TpM6Yj5l",
        "outputId": "6fc09c52-d200-4b17-f521-58b3be62a272"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss 80.262\n",
            "Epoch 2, Loss 58.052\n",
            "Epoch 3, Loss 54.556\n",
            "Epoch 4, Loss 51.738\n",
            "Epoch 5, Loss 49.206\n",
            "Epoch 6, Loss 46.916\n",
            "Epoch 7, Loss 44.865\n",
            "Epoch 8, Loss 43.304\n",
            "Epoch 9, Loss 41.819\n",
            "Epoch 10, Loss 40.591\n"
          ]
        }
      ]
    }
  ]
}